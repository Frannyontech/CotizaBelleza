import time
import re
import os
import json
from typing import List, Optional, Dict
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup

class MaicaoProduct:
    def __init__(self, nombre: str, marca: str, precio: float, 
                 categoria: str, stock: str, url: str = "", imagen: str = ""):
        self.nombre = nombre
        self.marca = marca
        self.precio = precio
        self.categoria = categoria
        self.stock = stock
        self.url = url
        self.imagen = imagen

    def to_dict(self) -> Dict:
        return {
            'nombre': self.nombre,
            'marca': self.marca,
            'precio': self.precio,
            'categoria': self.categoria,
            'stock': self.stock,
            'url': self.url,
            'imagen': self.imagen
        }

    def __str__(self) -> str:
        return f"{self.nombre} - {self.marca} - ${self.precio}"

class MaicaoSeleniumScraper:
    def __init__(self, headless: bool = True):
        self.driver = None
        self.setup_driver(headless)
        
        # Lista de marcas conocidas para identificaci√≥n
        self.marcas_conocidas = [
            'MAYBELLINE', 'REVLON', 'L\'OREAL', 'LOREAL', 'NYX', 'ESSENCE', 
            'CATRICE', 'COVERGIRL', 'RIMMEL', 'BOURJOIS', 'MILANI', 
            'WET N WILD', 'SKIN1004', 'MIXSOON', 'NEUTROGENA', 'TOCOBO', 
            'NIVEA', 'KIKO', 'CLINIQUE', 'ESTEE LAUDER', 'LANCOME', 
            'DIOR', 'CHANEL', 'MAC', 'URBAN DECAY', 'TOO FACED', 'BENEFIT',
            'THE ORDINARY', 'CERAVE', 'LA ROCHE POSAY', 'VICHY', 'EUCERIN',
            'GARNIER', 'OLAY', 'POND\'S', 'AVEENO', 'DOVE', 'MAICAO'
        ]

    def setup_driver(self, headless: bool):
        options = Options()
        if headless:
            options.add_argument('--headless=new')
        
        # Configuraci√≥n optimizada
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--disable-web-security')
        options.add_argument('--disable-features=VizDisplayCompositor')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-plugins')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # Configuraciones experimentales anti-detecci√≥n
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # Logging reducido
        options.add_argument('--log-level=3')
        options.add_argument('--silent')
        options.add_argument('--disable-logging')
        
        self.driver = webdriver.Chrome(options=options)
        
        # Ocultar propiedades de webdriver
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    def _detect_total_pages(self, categoria_url: str) -> int:
        """Detecta el n√∫mero total de p√°ginas disponibles"""
        try:
            self.driver.get(categoria_url)
            time.sleep(5)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # Buscar elementos de paginaci√≥n
            pagination_elements = soup.select('.pagination')
            if pagination_elements:
                # Buscar el texto de paginaci√≥n como "1234...109"
                pagination_text = pagination_elements[0].get_text(strip=True)
                
                # Extraer el √∫ltimo n√∫mero que indica el total de p√°ginas
                numbers = re.findall(r'\d+', pagination_text)
                if numbers:
                    total_pages = int(numbers[-1])  # El √∫ltimo n√∫mero suele ser el total
                    print(f"Total de p√°ginas detectadas: {total_pages}")
                    return total_pages
            
            # Fallback: buscar botones de p√°gina numerados
            page_buttons = soup.select('.btn-page, [class*="page"]')
            page_numbers = []
            for button in page_buttons:
                text = button.get_text(strip=True)
                if text.isdigit():
                    page_numbers.append(int(text))
            
            if page_numbers:
                total_pages = max(page_numbers)
                print(f"Total de p√°ginas detectadas (fallback): {total_pages}")
                return total_pages
            
            print("No se pudo detectar el n√∫mero total de p√°ginas, usando 1")
            return 1
            
        except Exception as e:
            print(f"Error detectando total de p√°ginas: {e}")
            return 1

    def _extract_product_price_from_detail(self, product_url: str) -> float:
        """Extrae el precio de la p√°gina de detalle del producto si no se encuentra en la lista"""
        try:
            # Abrir nueva pesta√±a
            self.driver.execute_script("window.open('');")
            self.driver.switch_to.window(self.driver.window_handles[1])
            
            # Navegar a la p√°gina del producto
            self.driver.get(product_url)
            time.sleep(3)
            
            # Buscar precio en la p√°gina de detalle
            precio_selectores = [
                '.price', '.product-price', '.price-value', '.price-current',
                '.precio', '.valor', '.cost', '.amount', '[class*="price"]',
                '.product-price-value', '.price-box .price', '.price-wrapper .price'
            ]
            
            for selector in precio_selectores:
                try:
                    precio_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    precio_texto = precio_element.text.strip()
                    if precio_texto and '$' in precio_texto:
                        precio = self._extraer_precio_del_texto(precio_texto)
                        if precio > 0:
                            return precio
                except NoSuchElementException:
                    continue
            
            # Buscar en el HTML completo si no se encuentra con selectores
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            texto_completo = soup.get_text()
            precio_match = re.search(r'\$[\s]*([0-9,.]+)', texto_completo)
            if precio_match:
                precio = self._extraer_precio_del_texto(precio_match.group())
                if precio > 0:
                    return precio
            
            return 0
            
        except Exception as e:
            print(f"Error extrayendo precio de detalle: {e}")
            return 0
        finally:
            # Cerrar pesta√±a y volver a la principal
            try:
                self.driver.close()
                self.driver.switch_to.window(self.driver.window_handles[0])
            except Exception:
                pass

    def _extraer_precio_del_texto(self, texto: str) -> float:
        """Extrae precio num√©rico del texto, retornando float CLP"""
        if not texto:
            return 0
        
        # Buscar patrones de precio chileno
        patterns = [
            r'\$[\s]*([0-9,.]+)',
            r'([0-9,.]+)[\s]*pesos',
            r'([0-9,.]+)[\s]*CLP'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, texto.replace(' ', ''))
            if match:
                precio_str = match.group(1).replace(',', '').replace('.', '')
                try:
                    precio = float(precio_str)
                    if precio > 0:
                        return precio
                except ValueError:
                    continue
        
        return 0

    def _extract_marca_from_element(self, product_element) -> str:
        """Extrae la marca del elemento de producto"""
        # Buscar espec√≠ficamente en enlaces de marca de Maicao
        marca_links = product_element.select('a[href*="/busqueda?q="]')
        for link in marca_links:
            marca_texto = link.get_text(strip=True)
            if marca_texto and marca_texto.upper() in self.marcas_conocidas:
                return marca_texto.upper()
        
        # Buscar en elementos con clase brand
        brand_elements = product_element.select('[class*="brand"]')
        for element in brand_elements:
            marca_texto = element.get_text(strip=True)
            if marca_texto and len(marca_texto) > 1:
                return marca_texto.upper()
        
        # Fallback: extraer de nombre
        return "MAICAO"
    
    def _extract_marca_from_name(self, nombre: str) -> str:
        """Extrae la marca del nombre del producto"""
        if not nombre:
            return "MAICAO"
        
        nombre_upper = nombre.upper()
        
        # Buscar marcas conocidas en el nombre
        for marca in self.marcas_conocidas:
            if marca in nombre_upper:
                return marca
        
        # Si no encuentra marca conocida, tomar la primera palabra
        primera_palabra = nombre.split()[0] if nombre.split() else "MAICAO"
        return primera_palabra.upper()

    def _determine_stock_status(self, product_element) -> str:
        """Determina el estado del stock del producto"""
        texto_elemento = product_element.get_text().lower()
        
        # Indicadores de productos agotados
        indicadores_agotado = [
            'agotado', 'sin stock', 'out of stock', 'no disponible', 
            'no available', 'sold out', 'fuera de stock'
        ]
        
        for indicador in indicadores_agotado:
            if indicador in texto_elemento:
                return "Out of stock"
        
        # Si no hay indicadores de agotado, asumir que est√° en stock
        return "In stock"

    def _extract_product_info_from_element(self, product_element, categoria: str) -> Optional[MaicaoProduct]:
        """Extrae informaci√≥n de un elemento de producto"""
        try:
            # Extraer nombre
            nombre = self._extract_nombre(product_element)
            if not nombre or len(nombre.strip()) < 3:
                return None
            
            # Extraer URL
            url = self._extract_url(product_element)
            if not url:
                return None
            
            # Extraer precio (primero de la lista, luego de detalle si es necesario)
            precio = self._extract_precio(product_element)
            if precio <= 0 and url:
                print(f"Precio no encontrado en lista para '{nombre}', extrayendo de p√°gina de detalle...")
                precio = self._extract_product_price_from_detail(url)
            
            # Extraer otros campos
            imagen = self._extract_imagen(product_element)
            marca = self._extract_marca_from_element(product_element)
            if marca == "MAICAO":  # Si no encontr√≥ marca en el elemento, buscar en el nombre
                marca = self._extract_marca_from_name(nombre)
            stock = self._determine_stock_status(product_element)
            
            return MaicaoProduct(
                nombre=nombre.strip(),
                marca=marca.strip(),
                precio=precio,
                categoria=categoria,
                stock=stock,
                url=url,
                imagen=imagen
            )
            
        except Exception as e:
            print(f"Error extrayendo producto: {e}")
            return None

    def _extract_nombre(self, product_element) -> str:
        """Extrae el nombre del producto"""
        # Buscar en enlaces espec√≠ficos que contienen el nombre del producto
        enlaces = product_element.select('a[href*=".html"]')
        for enlace in enlaces:
            texto = enlace.get_text(strip=True)
            if texto and len(texto) > 3 and self._es_texto_valido(texto):
                # Evitar texto de marca solamente
                if texto.upper() not in self.marcas_conocidas:
                    return self._limpiar_nombre(texto)
        
        # Fallback: buscar en selectores tradicionales
        nombre_selectores = [
            '.product-name', '.product-title', '.product-item-name',
            'h2', 'h3', 'h4', '.item-name', '.title', '.name',
            'a[title]', '.product-name a', '.product-title a',
            '[class*="title"]', '[class*="name"]'
        ]
        
        for selector in nombre_selectores:
            elementos = product_element.select(selector)
            for elemento in elementos:
                if selector == 'a[title]':
                    texto = elemento.get('title', '').strip()
                else:
                    texto = elemento.get_text(strip=True)
                
                if texto and len(texto) > 3 and self._es_texto_valido(texto):
                    return self._limpiar_nombre(texto)
        
        return ""

    def _extract_precio(self, product_element) -> float:
        """Extrae el precio del elemento de producto"""
        # Buscar espec√≠ficamente en la clase .sales que contiene el precio en Maicao
        precio_selectores = [
            '.sales',  # Selector espec√≠fico de Maicao
            '.price', '.product-price', '.price-value', '.price-current',
            '.precio', '.valor', '.cost', '.amount', '[class*="price"]',
            '.price-box .price', '.price-wrapper .price'
        ]
        
        for selector in precio_selectores:
            elementos = product_element.select(selector)
            for elemento in elementos:
                texto = elemento.get_text(strip=True)
                if texto and '$' in texto:
                    precio = self._extraer_precio_del_texto(texto)
                    if precio > 0:
                        return precio
        
        return 0

    def _extract_url(self, product_element) -> str:
        """Extrae la URL del producto"""
        # Buscar espec√≠ficamente enlaces que contengan .html (productos de Maicao)
        enlaces = product_element.select('a[href*=".html"]')
        for enlace in enlaces:
            href = enlace.get('href', '')
            if href:
                if href.startswith('http'):
                    return href
                elif href.startswith('/'):
                    return f"https://www.maicao.cl{href}"
        
        # Fallback: buscar cualquier enlace
        enlaces = product_element.select('a[href]')
        for enlace in enlaces:
            href = enlace.get('href', '')
            if href and '.html' in href:
                if href.startswith('http'):
                    return href
                elif href.startswith('/'):
                    return f"https://www.maicao.cl{href}"
        return ""

    def _extract_imagen(self, product_element) -> str:
        """Extrae la URL de la imagen del producto"""
        # Buscar espec√≠ficamente im√°genes del producto (no iconos o badges)
        imagenes = product_element.select('img[alt*="imagen"], img[src*="large/"]')
        for img in imagenes:
            src = img.get('src', '')
            if src and 'large/' in src and not src.startswith('data:'):
                if src.startswith('http'):
                    return src
                elif src.startswith('/'):
                    return f"https://www.maicao.cl{src}"
        
        # Fallback: buscar cualquier imagen con src v√°lido
        img_selectores = [
            'img[src]', 'img[data-src]', 'img[data-lazy]', 'img[data-original]'
        ]
        
        for selector in img_selectores:
            imagenes = product_element.select(selector)
            for img in imagenes:
                src = (img.get('src') or img.get('data-src') or 
                      img.get('data-lazy') or img.get('data-original', ''))
                
                # Filtrar iconos y badges
                if (src and not src.startswith('data:') and 
                    'cart-icon' not in src and 'ribbon-' not in src and 
                    len(src) > 20):
                    if src.startswith('http'):
                        return src
                    elif src.startswith('/'):
                        return f"https://www.maicao.cl{src}"
        
        return ""

    def _es_texto_valido(self, texto: str) -> bool:
        """Valida que el texto sea un nombre de producto v√°lido"""
        # Eliminar caracteres especiales para verificaci√≥n
        texto_limpio = re.sub(r'[^\w\s]', '', texto).strip()
        
        # Verificar que no contenga solo n√∫meros
        if texto_limpio.isdigit():
            return False
        
        # Verificar que tenga letras
        if not re.search(r'[a-zA-Z]', texto_limpio):
            return False
        
        # Verificar que no sea un precio
        if re.search(r'\$\s*\d+', texto):
            return False
        
        return True

    def _limpiar_nombre(self, nombre: str) -> str:
        """Limpia y normaliza el nombre del producto"""
        # Eliminar patrones de precio y caracteres especiales innecesarios
        nombre = re.sub(r'\$\s*\d+', '', nombre)
        nombre = re.sub(r'\d+\s*-\s*\d+', '', nombre)
        nombre = re.sub(r'[^\w\s\-\.]', ' ', nombre)
        nombre = re.sub(r'\s+', ' ', nombre)
        return nombre.strip()

    def scrape_category(self, categoria_url: str, categoria_nombre: str, max_pages: int = None) -> List[MaicaoProduct]:
        """Scrapea una categor√≠a espec√≠fica de Maicao usando paginaci√≥n"""
        # Mostrar nombre amigable para logs
        nombre_display = "cuidado de la piel" if categoria_nombre == "skincare" else categoria_nombre
        print(f"Scrapeando categor√≠a: {nombre_display}")
        print(f"URL: {categoria_url}")
        
        try:
            # Detectar el total de p√°ginas disponibles
            if max_pages is None:
                total_pages = self._detect_total_pages(categoria_url)
            else:
                total_pages = max_pages
            
            print(f"Scrapeando {total_pages} p√°ginas de productos...")
            
            todos_productos = []
            productos_por_pagina = 18  # Maicao muestra 18 productos por p√°gina
            
            for pagina in range(1, total_pages + 1):
                # Calcular el par√°metro start (empieza en 0 para la primera p√°gina)
                start = (pagina - 1) * productos_por_pagina
                url_pagina = f"{categoria_url}?start={start}" if start > 0 else categoria_url
                
                print(f"\nP√°gina {pagina}/{total_pages}: {url_pagina}")
                
                try:
                    # Navegar a la p√°gina espec√≠fica
                    self.driver.get(url_pagina)
                    time.sleep(3)  # Tiempo para carga
                    
                    # Obtener HTML de la p√°gina
                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                    
                    # Buscar elementos de productos
                    elementos_productos = soup.select('.product.product-wrapper')
                    
                    if not elementos_productos:
                        print(f"No se encontraron productos en la p√°gina {pagina}, terminando...")
                        break
                    
                    print(f"Productos encontrados en p√°gina {pagina}: {len(elementos_productos)}")
                    
                    # Procesar productos de esta p√°gina
                    productos_pagina = []
                    for i, elemento in enumerate(elementos_productos):
                        try:
                            producto = self._extract_product_info_from_element(elemento, categoria_nombre)
                            
                            if producto and producto.nombre:
                                productos_pagina.append(producto)
                            
                        except Exception as e:
                            print(f"Error procesando producto {i+1} en p√°gina {pagina}: {e}")
                    
                    # Agregar productos de esta p√°gina al total
                    todos_productos.extend(productos_pagina)
                    
                    print(f"Productos v√°lidos extra√≠dos de p√°gina {pagina}: {len(productos_pagina)}")
                    
                    # Progreso cada 10 p√°ginas
                    if pagina % 10 == 0:
                        print(f"Progreso: {pagina}/{total_pages} p√°ginas procesadas. Total productos: {len(todos_productos)}")
                    
                    # Pausa entre p√°ginas para evitar sobrecarga del servidor
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"Error en p√°gina {pagina}: {e}")
                    continue
            
            # Eliminar duplicados basados en URL (por si acaso)
            productos_unicos = []
            urls_vistas = set()
            
            for producto in todos_productos:
                if producto.url not in urls_vistas:
                    urls_vistas.add(producto.url)
                    productos_unicos.append(producto)
            
            print(f"\nCategor√≠a {nombre_display} completada:")
            print(f"- Total p√°ginas procesadas: {total_pages}")
            print(f"- Productos extra√≠dos (con duplicados): {len(todos_productos)}")
            print(f"- Productos √∫nicos finales: {len(productos_unicos)}")
            
            return productos_unicos
            
        except Exception as e:
            print(f"Error scrapeando categor√≠a {categoria_nombre}: {e}")
            return []

    def close(self):
        """Cierra el driver de Selenium"""
        if self.driver:
            self.driver.quit()

def scrape_maicao_all_categories(headless: bool = True, max_pages_per_category: int = None) -> Dict:
    """Funci√≥n principal que scrapea todas las categor√≠as de Maicao"""
    
    # URLs de las categor√≠as
    categorias = {
        "maquillaje": "https://www.maicao.cl/maquillaje/",
        "skincare": "https://www.maicao.cl/cuidado-de-la-piel/"
    }
    
    scraper = MaicaoSeleniumScraper(headless=headless)
    resultados = {}
    
    try:
        for categoria_nombre, categoria_url in categorias.items():
            print(f"\n{'='*50}")
            print(f"SCRAPEANDO CATEGOR√çA: {categoria_nombre.upper()}")
            print(f"{'='*50}")
            
            productos_categoria = scraper.scrape_category(categoria_url, categoria_nombre, max_pages_per_category)
            
            # Convertir productos a diccionarios y agregar a resultados
            productos_dict = [producto.to_dict() for producto in productos_categoria]
            resultados[categoria_nombre] = {
                'cantidad': len(productos_categoria),
                'productos': productos_dict
            }
            
            nombre_display = "cuidado de la piel" if categoria_nombre == "skincare" else categoria_nombre
            print(f"Categor√≠a {nombre_display}: {len(productos_categoria)} productos extra√≠dos")
            
            # Pausa entre categor√≠as
            time.sleep(3)
        
        # Crear estructura final consistente con otros scrapers
        from datetime import datetime
        data_completa = {
            'fecha_extraccion': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_productos': sum(cat['cantidad'] for cat in resultados.values()),
            **resultados
        }
        
        # Estad√≠sticas finales
        print(f"\n{'='*50}")
        print("ESTAD√çSTICAS FINALES")
        print(f"{'='*50}")
        
        for categoria, datos in resultados.items():
            nombre_display = "cuidado de la piel" if categoria == "skincare" else categoria
            print(f"Categor√≠a '{nombre_display}': {datos['cantidad']} productos")
        
        print(f"TOTAL FINAL: {data_completa['total_productos']} productos")
        
        # Guardar solo archivos separados por categor√≠a
        archivos_guardados = guardar_resultados_por_categoria_maicao(data_completa, "maicao")
        print(f"\n=== RESUMEN MAICAO ===")
        print(f"Total archivos generados: {len(archivos_guardados)}")
        for archivo in archivos_guardados:
            print(f"  - {archivo}")
        
        return data_completa
        
    except Exception as e:
        print(f"Error general en el scraping: {e}")
        return {}
    
    finally:
        scraper.close()

def guardar_resultados_por_categoria_maicao(resultados, tienda_prefix="maicao"):
    """
    Guarda los resultados en archivos JSON separados por categor√≠a para Maicao
    """
    # Obtener la ruta absoluta desde el directorio del script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    data_dir = os.path.join(project_root, "data")
    os.makedirs(data_dir, exist_ok=True)
    
    archivos_guardados = []
    
    # Extraer metadatos generales
    metadatos = {
        'fecha_extraccion': resultados.get('fecha_extraccion'),
        'tienda': tienda_prefix.upper()
    }
    
    # Guardar cada categor√≠a en un archivo separado
    for categoria, datos_categoria in resultados.items():
        if categoria in ['fecha_extraccion', 'total_productos']:
            continue  # Saltar metadatos
            
        # Crear estructura para archivo individual
        estructura_categoria = {
            **metadatos,
            'categoria': categoria,
            'total_productos': datos_categoria['cantidad'],
            'productos': datos_categoria['productos']
        }
        
        # Nombre del archivo: tienda_categoria.json
        nombre_archivo = f"{tienda_prefix}_{categoria}.json"
        ruta_archivo = os.path.join(data_dir, nombre_archivo)
        
        # Guardar archivo
        with open(ruta_archivo, 'w', encoding='utf-8') as f:
            json.dump(estructura_categoria, f, ensure_ascii=False, indent=2)
        
        print(f"Categor√≠a '{categoria}' guardada en: {ruta_archivo}")
        archivos_guardados.append(ruta_archivo)
    
    return archivos_guardados

if __name__ == "__main__":
    print("=== SCRAPER MAICAO - ARCHIVOS SEPARADOS POR CATEGOR√çA ===")
    print("Iniciando scraping de Maicao con archivos separados...")
    
    # Configuraci√≥n
    max_pages = 2  # Cambiar a None para scrapear todas las p√°ginas
    headless = True  # Cambiar a False si quieres ver el navegador
    
    if max_pages:
        print(f"MODO PRUEBA: Limitado a {max_pages} p√°ginas por categor√≠a")
    else:
        print("MODO COMPLETO: Scrapeando todas las p√°ginas disponibles")
    
    try:
        resultado = scrape_maicao_all_categories(headless=headless, max_pages_per_category=max_pages)
        
        print(f"\nüéâ SCRAPING COMPLETADO")
        total = resultado.get('total_productos', 0) if isinstance(resultado, dict) else len(resultado)
        print(f"Total productos extra√≠dos: {total}")
        
        if isinstance(resultado, dict):
            for categoria, datos in resultado.items():
                if categoria not in ['fecha_extraccion', 'total_productos']:
                    print(f"  {categoria}: {datos['cantidad']} productos")
        
    except Exception as e:
        print(f"‚ùå Error durante el scraping: {e}")
        import traceback
        traceback.print_exc()
